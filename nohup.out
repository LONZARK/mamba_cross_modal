/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Traceback (most recent call last):
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 261, in <module>
    main()
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 232, in main
    train(args, model, train_loader, optimizer, criterion, args.mamba_flag, args.crossmodal, epoch=epoch)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 23, in train
    output, a_prob, v_prob, _ = model(audio, video, video_st, mamba_flag, crossmodal)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 229, in forward
    x1, x2 = self.hat_encoder(x1, x2)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 68, in forward
    output_a = self.layers[i](src_a, src_v,  src_mask=mask, 
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 174, in forward
    src_q = src_q + self.dropout11(src1) + self.dropout12(src2)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/functional.py", line 1266, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple
/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Traceback (most recent call last):
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 261, in <module>
    main()
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 256, in main
    model.load_state_dict(torch.load(args.model_save_dir + args.checkpoint + ".pt"))
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for MMIL_Net:
	Missing key(s) in state_dict: "han_layer.self_attn.in_proj_weight", "han_layer.self_attn.in_proj_bias", "han_layer.self_attn.out_proj.weight", "han_layer.self_attn.out_proj.bias", "han_layer.cm_attn.in_proj_weight", "han_layer.cm_attn.in_proj_bias", "han_layer.cm_attn.out_proj.weight", "han_layer.cm_attn.out_proj.bias", "han_layer.linear1.weight", "han_layer.linear1.bias", "han_layer.linear2.weight", "han_layer.linear2.bias", "han_layer.norm1.weight", "han_layer.norm1.bias", "han_layer.norm2.weight", "han_layer.norm2.bias", "han_layer.mamba.layers.0.A_log_a", "han_layer.mamba.layers.0.D_a", "han_layer.mamba.layers.0.A_log_v", "han_layer.mamba.layers.0.D_v", "han_layer.mamba.layers.0.in_proj_a.weight", "han_layer.mamba.layers.0.in_proj_v.weight", "han_layer.mamba.layers.0.conv1d_a.weight", "han_layer.mamba.layers.0.conv1d_a.bias", "han_layer.mamba.layers.0.conv1d_v.weight", "han_layer.mamba.layers.0.conv1d_v.bias", "han_layer.mamba.layers.0.x_proj_a.weight", "han_layer.mamba.layers.0.x_proj_v.weight", "han_layer.mamba.layers.0.dt_proj_a.weight", "han_layer.mamba.layers.0.dt_proj_a.bias", "han_layer.mamba.layers.0.dt_proj_v.weight", "han_layer.mamba.layers.0.dt_proj_v.bias", "han_layer.mamba.layers.0.attention.in_proj_weight", "han_layer.mamba.layers.0.attention.in_proj_bias", "han_layer.mamba.layers.0.attention.out_proj.weight", "han_layer.mamba.layers.0.attention.out_proj.bias", "han_layer.mamba.layers.0.out_proj_a.weight", "han_layer.mamba.layers.0.out_proj_v.weight", "han_layer.mamba.norm1.weight", "han_layer.mamba.norm1.bias", "han_layer.mamba.norm2.weight", "han_layer.mamba.norm2.bias". 
	Unexpected key(s) in state_dict: "mamba_encoder.layers.0.A_log_a", "mamba_encoder.layers.0.D_a", "mamba_encoder.layers.0.A_log_v", "mamba_encoder.layers.0.D_v", "mamba_encoder.layers.0.in_proj_a.weight", "mamba_encoder.layers.0.in_proj_v.weight", "mamba_encoder.layers.0.conv1d_a.weight", "mamba_encoder.layers.0.conv1d_a.bias", "mamba_encoder.layers.0.conv1d_v.weight", "mamba_encoder.layers.0.conv1d_v.bias", "mamba_encoder.layers.0.x_proj_a.weight", "mamba_encoder.layers.0.x_proj_v.weight", "mamba_encoder.layers.0.dt_proj_a.weight", "mamba_encoder.layers.0.dt_proj_a.bias", "mamba_encoder.layers.0.dt_proj_v.weight", "mamba_encoder.layers.0.dt_proj_v.bias", "mamba_encoder.layers.0.attention.in_proj_weight", "mamba_encoder.layers.0.attention.in_proj_bias", "mamba_encoder.layers.0.attention.out_proj.weight", "mamba_encoder.layers.0.attention.out_proj.bias", "mamba_encoder.layers.0.out_proj_a.weight", "mamba_encoder.layers.0.out_proj_v.weight", "mamba_encoder.layers.1.A_log_a", "mamba_encoder.layers.1.D_a", "mamba_encoder.layers.1.A_log_v", "mamba_encoder.layers.1.D_v", "mamba_encoder.layers.1.in_proj_a.weight", "mamba_encoder.layers.1.in_proj_v.weight", "mamba_encoder.layers.1.conv1d_a.weight", "mamba_encoder.layers.1.conv1d_a.bias", "mamba_encoder.layers.1.conv1d_v.weight", "mamba_encoder.layers.1.conv1d_v.bias", "mamba_encoder.layers.1.x_proj_a.weight", "mamba_encoder.layers.1.x_proj_v.weight", "mamba_encoder.layers.1.dt_proj_a.weight", "mamba_encoder.layers.1.dt_proj_a.bias", "mamba_encoder.layers.1.dt_proj_v.weight", "mamba_encoder.layers.1.dt_proj_v.bias", "mamba_encoder.layers.1.attention.in_proj_weight", "mamba_encoder.layers.1.attention.in_proj_bias", "mamba_encoder.layers.1.attention.out_proj.weight", "mamba_encoder.layers.1.attention.out_proj.bias", "mamba_encoder.layers.1.out_proj_a.weight", "mamba_encoder.layers.1.out_proj_v.weight", "mamba_encoder.layers.2.A_log_a", "mamba_encoder.layers.2.D_a", "mamba_encoder.layers.2.A_log_v", "mamba_encoder.layers.2.D_v", "mamba_encoder.layers.2.in_proj_a.weight", "mamba_encoder.layers.2.in_proj_v.weight", "mamba_encoder.layers.2.conv1d_a.weight", "mamba_encoder.layers.2.conv1d_a.bias", "mamba_encoder.layers.2.conv1d_v.weight", "mamba_encoder.layers.2.conv1d_v.bias", "mamba_encoder.layers.2.x_proj_a.weight", "mamba_encoder.layers.2.x_proj_v.weight", "mamba_encoder.layers.2.dt_proj_a.weight", "mamba_encoder.layers.2.dt_proj_a.bias", "mamba_encoder.layers.2.dt_proj_v.weight", "mamba_encoder.layers.2.dt_proj_v.bias", "mamba_encoder.layers.2.attention.in_proj_weight", "mamba_encoder.layers.2.attention.in_proj_bias", "mamba_encoder.layers.2.attention.out_proj.weight", "mamba_encoder.layers.2.attention.out_proj.bias", "mamba_encoder.layers.2.out_proj_a.weight", "mamba_encoder.layers.2.out_proj_v.weight", "mamba_encoder.layers.3.A_log_a", "mamba_encoder.layers.3.D_a", "mamba_encoder.layers.3.A_log_v", "mamba_encoder.layers.3.D_v", "mamba_encoder.layers.3.in_proj_a.weight", "mamba_encoder.layers.3.in_proj_v.weight", "mamba_encoder.layers.3.conv1d_a.weight", "mamba_encoder.layers.3.conv1d_a.bias", "mamba_encoder.layers.3.conv1d_v.weight", "mamba_encoder.layers.3.conv1d_v.bias", "mamba_encoder.layers.3.x_proj_a.weight", "mamba_encoder.layers.3.x_proj_v.weight", "mamba_encoder.layers.3.dt_proj_a.weight", "mamba_encoder.layers.3.dt_proj_a.bias", "mamba_encoder.layers.3.dt_proj_v.weight", "mamba_encoder.layers.3.dt_proj_v.bias", "mamba_encoder.layers.3.attention.in_proj_weight", "mamba_encoder.layers.3.attention.in_proj_bias", "mamba_encoder.layers.3.attention.out_proj.weight", "mamba_encoder.layers.3.attention.out_proj.bias", "mamba_encoder.layers.3.out_proj_a.weight", "mamba_encoder.layers.3.out_proj_v.weight", "mamba_encoder.layers.4.A_log_a", "mamba_encoder.layers.4.D_a", "mamba_encoder.layers.4.A_log_v", "mamba_encoder.layers.4.D_v", "mamba_encoder.layers.4.in_proj_a.weight", "mamba_encoder.layers.4.in_proj_v.weight", "mamba_encoder.layers.4.conv1d_a.weight", "mamba_encoder.layers.4.conv1d_a.bias", "mamba_encoder.layers.4.conv1d_v.weight", "mamba_encoder.layers.4.conv1d_v.bias", "mamba_encoder.layers.4.x_proj_a.weight", "mamba_encoder.layers.4.x_proj_v.weight", "mamba_encoder.layers.4.dt_proj_a.weight", "mamba_encoder.layers.4.dt_proj_a.bias", "mamba_encoder.layers.4.dt_proj_v.weight", "mamba_encoder.layers.4.dt_proj_v.bias", "mamba_encoder.layers.4.attention.in_proj_weight", "mamba_encoder.layers.4.attention.in_proj_bias", "mamba_encoder.layers.4.attention.out_proj.weight", "mamba_encoder.layers.4.attention.out_proj.bias", "mamba_encoder.layers.4.out_proj_a.weight", "mamba_encoder.layers.4.out_proj_v.weight", "mamba_encoder.layers.5.A_log_a", "mamba_encoder.layers.5.D_a", "mamba_encoder.layers.5.A_log_v", "mamba_encoder.layers.5.D_v", "mamba_encoder.layers.5.in_proj_a.weight", "mamba_encoder.layers.5.in_proj_v.weight", "mamba_encoder.layers.5.conv1d_a.weight", "mamba_encoder.layers.5.conv1d_a.bias", "mamba_encoder.layers.5.conv1d_v.weight", "mamba_encoder.layers.5.conv1d_v.bias", "mamba_encoder.layers.5.x_proj_a.weight", "mamba_encoder.layers.5.x_proj_v.weight", "mamba_encoder.layers.5.dt_proj_a.weight", "mamba_encoder.layers.5.dt_proj_a.bias", "mamba_encoder.layers.5.dt_proj_v.weight", "mamba_encoder.layers.5.dt_proj_v.bias", "mamba_encoder.layers.5.attention.in_proj_weight", "mamba_encoder.layers.5.attention.in_proj_bias", "mamba_encoder.layers.5.attention.out_proj.weight", "mamba_encoder.layers.5.attention.out_proj.bias", "mamba_encoder.layers.5.out_proj_a.weight", "mamba_encoder.layers.5.out_proj_v.weight", "mamba_encoder.layers.6.A_log_a", "mamba_encoder.layers.6.D_a", "mamba_encoder.layers.6.A_log_v", "mamba_encoder.layers.6.D_v", "mamba_encoder.layers.6.in_proj_a.weight", "mamba_encoder.layers.6.in_proj_v.weight", "mamba_encoder.layers.6.conv1d_a.weight", "mamba_encoder.layers.6.conv1d_a.bias", "mamba_encoder.layers.6.conv1d_v.weight", "mamba_encoder.layers.6.conv1d_v.bias", "mamba_encoder.layers.6.x_proj_a.weight", "mamba_encoder.layers.6.x_proj_v.weight", "mamba_encoder.layers.6.dt_proj_a.weight", "mamba_encoder.layers.6.dt_proj_a.bias", "mamba_encoder.layers.6.dt_proj_v.weight", "mamba_encoder.layers.6.dt_proj_v.bias", "mamba_encoder.layers.6.attention.in_proj_weight", "mamba_encoder.layers.6.attention.in_proj_bias", "mamba_encoder.layers.6.attention.out_proj.weight", "mamba_encoder.layers.6.attention.out_proj.bias", "mamba_encoder.layers.6.out_proj_a.weight", "mamba_encoder.layers.6.out_proj_v.weight", "mamba_encoder.layers.7.A_log_a", "mamba_encoder.layers.7.D_a", "mamba_encoder.layers.7.A_log_v", "mamba_encoder.layers.7.D_v", "mamba_encoder.layers.7.in_proj_a.weight", "mamba_encoder.layers.7.in_proj_v.weight", "mamba_encoder.layers.7.conv1d_a.weight", "mamba_encoder.layers.7.conv1d_a.bias", "mamba_encoder.layers.7.conv1d_v.weight", "mamba_encoder.layers.7.conv1d_v.bias", "mamba_encoder.layers.7.x_proj_a.weight", "mamba_encoder.layers.7.x_proj_v.weight", "mamba_encoder.layers.7.dt_proj_a.weight", "mamba_encoder.layers.7.dt_proj_a.bias", "mamba_encoder.layers.7.dt_proj_v.weight", "mamba_encoder.layers.7.dt_proj_v.bias", "mamba_encoder.layers.7.attention.in_proj_weight", "mamba_encoder.layers.7.attention.in_proj_bias", "mamba_encoder.layers.7.attention.out_proj.weight", "mamba_encoder.layers.7.attention.out_proj.bias", "mamba_encoder.layers.7.out_proj_a.weight", "mamba_encoder.layers.7.out_proj_v.weight", "mamba_encoder.layers.8.A_log_a", "mamba_encoder.layers.8.D_a", "mamba_encoder.layers.8.A_log_v", "mamba_encoder.layers.8.D_v", "mamba_encoder.layers.8.in_proj_a.weight", "mamba_encoder.layers.8.in_proj_v.weight", "mamba_encoder.layers.8.conv1d_a.weight", "mamba_encoder.layers.8.conv1d_a.bias", "mamba_encoder.layers.8.conv1d_v.weight", "mamba_encoder.layers.8.conv1d_v.bias", "mamba_encoder.layers.8.x_proj_a.weight", "mamba_encoder.layers.8.x_proj_v.weight", "mamba_encoder.layers.8.dt_proj_a.weight", "mamba_encoder.layers.8.dt_proj_a.bias", "mamba_encoder.layers.8.dt_proj_v.weight", "mamba_encoder.layers.8.dt_proj_v.bias", "mamba_encoder.layers.8.attention.in_proj_weight", "mamba_encoder.layers.8.attention.in_proj_bias", "mamba_encoder.layers.8.attention.out_proj.weight", "mamba_encoder.layers.8.attention.out_proj.bias", "mamba_encoder.layers.8.out_proj_a.weight", "mamba_encoder.layers.8.out_proj_v.weight", "mamba_encoder.layers.9.A_log_a", "mamba_encoder.layers.9.D_a", "mamba_encoder.layers.9.A_log_v", "mamba_encoder.layers.9.D_v", "mamba_encoder.layers.9.in_proj_a.weight", "mamba_encoder.layers.9.in_proj_v.weight", "mamba_encoder.layers.9.conv1d_a.weight", "mamba_encoder.layers.9.conv1d_a.bias", "mamba_encoder.layers.9.conv1d_v.weight", "mamba_encoder.layers.9.conv1d_v.bias", "mamba_encoder.layers.9.x_proj_a.weight", "mamba_encoder.layers.9.x_proj_v.weight", "mamba_encoder.layers.9.dt_proj_a.weight", "mamba_encoder.layers.9.dt_proj_a.bias", "mamba_encoder.layers.9.dt_proj_v.weight", "mamba_encoder.layers.9.dt_proj_v.bias", "mamba_encoder.layers.9.attention.in_proj_weight", "mamba_encoder.layers.9.attention.in_proj_bias", "mamba_encoder.layers.9.attention.out_proj.weight", "mamba_encoder.layers.9.attention.out_proj.bias", "mamba_encoder.layers.9.out_proj_a.weight", "mamba_encoder.layers.9.out_proj_v.weight", "mamba_encoder.norm1.weight", "mamba_encoder.norm1.bias", "mamba_encoder.norm2.weight", "mamba_encoder.norm2.bias", "hat_encoder.layers.0.mamba.layers.1.A_log_a", "hat_encoder.layers.0.mamba.layers.1.D_a", "hat_encoder.layers.0.mamba.layers.1.A_log_v", "hat_encoder.layers.0.mamba.layers.1.D_v", "hat_encoder.layers.0.mamba.layers.1.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.1.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.1.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.1.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.1.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.1.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.1.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.1.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.1.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.A_log_a", "hat_encoder.layers.0.mamba.layers.2.D_a", "hat_encoder.layers.0.mamba.layers.2.A_log_v", "hat_encoder.layers.0.mamba.layers.2.D_v", "hat_encoder.layers.0.mamba.layers.2.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.2.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.2.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.2.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.2.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.2.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.2.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.2.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.2.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.A_log_a", "hat_encoder.layers.0.mamba.layers.3.D_a", "hat_encoder.layers.0.mamba.layers.3.A_log_v", "hat_encoder.layers.0.mamba.layers.3.D_v", "hat_encoder.layers.0.mamba.layers.3.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.3.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.3.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.3.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.3.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.3.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.3.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.3.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.3.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.A_log_a", "hat_encoder.layers.0.mamba.layers.4.D_a", "hat_encoder.layers.0.mamba.layers.4.A_log_v", "hat_encoder.layers.0.mamba.layers.4.D_v", "hat_encoder.layers.0.mamba.layers.4.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.4.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.4.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.4.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.4.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.4.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.4.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.4.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.4.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.A_log_a", "hat_encoder.layers.0.mamba.layers.5.D_a", "hat_encoder.layers.0.mamba.layers.5.A_log_v", "hat_encoder.layers.0.mamba.layers.5.D_v", "hat_encoder.layers.0.mamba.layers.5.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.5.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.5.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.5.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.5.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.5.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.5.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.5.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.5.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.A_log_a", "hat_encoder.layers.0.mamba.layers.6.D_a", "hat_encoder.layers.0.mamba.layers.6.A_log_v", "hat_encoder.layers.0.mamba.layers.6.D_v", "hat_encoder.layers.0.mamba.layers.6.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.6.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.6.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.6.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.6.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.6.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.6.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.6.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.6.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.A_log_a", "hat_encoder.layers.0.mamba.layers.7.D_a", "hat_encoder.layers.0.mamba.layers.7.A_log_v", "hat_encoder.layers.0.mamba.layers.7.D_v", "hat_encoder.layers.0.mamba.layers.7.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.7.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.7.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.7.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.7.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.7.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.7.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.7.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.7.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.A_log_a", "hat_encoder.layers.0.mamba.layers.8.D_a", "hat_encoder.layers.0.mamba.layers.8.A_log_v", "hat_encoder.layers.0.mamba.layers.8.D_v", "hat_encoder.layers.0.mamba.layers.8.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.8.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.8.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.8.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.8.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.8.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.8.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.8.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.8.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.A_log_a", "hat_encoder.layers.0.mamba.layers.9.D_a", "hat_encoder.layers.0.mamba.layers.9.A_log_v", "hat_encoder.layers.0.mamba.layers.9.D_v", "hat_encoder.layers.0.mamba.layers.9.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.9.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.9.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.9.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.9.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.9.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.9.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.9.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.9.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.out_proj_v.weight". 
/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Traceback (most recent call last):
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 261, in <module>
    main()
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 232, in main
    train(args, model, train_loader, optimizer, criterion, args.mamba_flag, args.crossmodal, epoch=epoch)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 23, in train
    output, a_prob, v_prob, _ = model(audio, video, video_st, mamba_flag, crossmodal)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 229, in forward
    x1, x2 = self.hat_encoder(x1, x2)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 68, in forward
    output_a = self.layers[i](src_a, src_v,  src_mask=mask, 
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/nets/net_audiovisual.py", line 174, in forward
    src_q = src_q + self.dropout11(src1) + self.dropout12(src2)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/functional.py", line 1266, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple
/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Traceback (most recent call last):
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 261, in <module>
    main()
  File "/home/jxl220096/code/ai_assignment/mamba_cross_modal/avvp_eccv20/main_avvp.py", line 256, in main
    model.load_state_dict(torch.load(args.model_save_dir + args.checkpoint + ".pt"))
  File "/home/jxl220096/code/.conda/bin/python/llp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for MMIL_Net:
	Missing key(s) in state_dict: "han_layer.self_attn.in_proj_weight", "han_layer.self_attn.in_proj_bias", "han_layer.self_attn.out_proj.weight", "han_layer.self_attn.out_proj.bias", "han_layer.cm_attn.in_proj_weight", "han_layer.cm_attn.in_proj_bias", "han_layer.cm_attn.out_proj.weight", "han_layer.cm_attn.out_proj.bias", "han_layer.linear1.weight", "han_layer.linear1.bias", "han_layer.linear2.weight", "han_layer.linear2.bias", "han_layer.norm1.weight", "han_layer.norm1.bias", "han_layer.norm2.weight", "han_layer.norm2.bias", "han_layer.mamba.layers.0.A_log_a", "han_layer.mamba.layers.0.D_a", "han_layer.mamba.layers.0.A_log_v", "han_layer.mamba.layers.0.D_v", "han_layer.mamba.layers.0.in_proj_a.weight", "han_layer.mamba.layers.0.in_proj_v.weight", "han_layer.mamba.layers.0.conv1d_a.weight", "han_layer.mamba.layers.0.conv1d_a.bias", "han_layer.mamba.layers.0.conv1d_v.weight", "han_layer.mamba.layers.0.conv1d_v.bias", "han_layer.mamba.layers.0.x_proj_a.weight", "han_layer.mamba.layers.0.x_proj_v.weight", "han_layer.mamba.layers.0.dt_proj_a.weight", "han_layer.mamba.layers.0.dt_proj_a.bias", "han_layer.mamba.layers.0.dt_proj_v.weight", "han_layer.mamba.layers.0.dt_proj_v.bias", "han_layer.mamba.layers.0.attention.in_proj_weight", "han_layer.mamba.layers.0.attention.in_proj_bias", "han_layer.mamba.layers.0.attention.out_proj.weight", "han_layer.mamba.layers.0.attention.out_proj.bias", "han_layer.mamba.layers.0.out_proj_a.weight", "han_layer.mamba.layers.0.out_proj_v.weight", "han_layer.mamba.norm1.weight", "han_layer.mamba.norm1.bias", "han_layer.mamba.norm2.weight", "han_layer.mamba.norm2.bias". 
	Unexpected key(s) in state_dict: "mamba_encoder.layers.0.A_log_a", "mamba_encoder.layers.0.D_a", "mamba_encoder.layers.0.A_log_v", "mamba_encoder.layers.0.D_v", "mamba_encoder.layers.0.in_proj_a.weight", "mamba_encoder.layers.0.in_proj_v.weight", "mamba_encoder.layers.0.conv1d_a.weight", "mamba_encoder.layers.0.conv1d_a.bias", "mamba_encoder.layers.0.conv1d_v.weight", "mamba_encoder.layers.0.conv1d_v.bias", "mamba_encoder.layers.0.x_proj_a.weight", "mamba_encoder.layers.0.x_proj_v.weight", "mamba_encoder.layers.0.dt_proj_a.weight", "mamba_encoder.layers.0.dt_proj_a.bias", "mamba_encoder.layers.0.dt_proj_v.weight", "mamba_encoder.layers.0.dt_proj_v.bias", "mamba_encoder.layers.0.attention.in_proj_weight", "mamba_encoder.layers.0.attention.in_proj_bias", "mamba_encoder.layers.0.attention.out_proj.weight", "mamba_encoder.layers.0.attention.out_proj.bias", "mamba_encoder.layers.0.out_proj_a.weight", "mamba_encoder.layers.0.out_proj_v.weight", "mamba_encoder.layers.1.A_log_a", "mamba_encoder.layers.1.D_a", "mamba_encoder.layers.1.A_log_v", "mamba_encoder.layers.1.D_v", "mamba_encoder.layers.1.in_proj_a.weight", "mamba_encoder.layers.1.in_proj_v.weight", "mamba_encoder.layers.1.conv1d_a.weight", "mamba_encoder.layers.1.conv1d_a.bias", "mamba_encoder.layers.1.conv1d_v.weight", "mamba_encoder.layers.1.conv1d_v.bias", "mamba_encoder.layers.1.x_proj_a.weight", "mamba_encoder.layers.1.x_proj_v.weight", "mamba_encoder.layers.1.dt_proj_a.weight", "mamba_encoder.layers.1.dt_proj_a.bias", "mamba_encoder.layers.1.dt_proj_v.weight", "mamba_encoder.layers.1.dt_proj_v.bias", "mamba_encoder.layers.1.attention.in_proj_weight", "mamba_encoder.layers.1.attention.in_proj_bias", "mamba_encoder.layers.1.attention.out_proj.weight", "mamba_encoder.layers.1.attention.out_proj.bias", "mamba_encoder.layers.1.out_proj_a.weight", "mamba_encoder.layers.1.out_proj_v.weight", "mamba_encoder.layers.2.A_log_a", "mamba_encoder.layers.2.D_a", "mamba_encoder.layers.2.A_log_v", "mamba_encoder.layers.2.D_v", "mamba_encoder.layers.2.in_proj_a.weight", "mamba_encoder.layers.2.in_proj_v.weight", "mamba_encoder.layers.2.conv1d_a.weight", "mamba_encoder.layers.2.conv1d_a.bias", "mamba_encoder.layers.2.conv1d_v.weight", "mamba_encoder.layers.2.conv1d_v.bias", "mamba_encoder.layers.2.x_proj_a.weight", "mamba_encoder.layers.2.x_proj_v.weight", "mamba_encoder.layers.2.dt_proj_a.weight", "mamba_encoder.layers.2.dt_proj_a.bias", "mamba_encoder.layers.2.dt_proj_v.weight", "mamba_encoder.layers.2.dt_proj_v.bias", "mamba_encoder.layers.2.attention.in_proj_weight", "mamba_encoder.layers.2.attention.in_proj_bias", "mamba_encoder.layers.2.attention.out_proj.weight", "mamba_encoder.layers.2.attention.out_proj.bias", "mamba_encoder.layers.2.out_proj_a.weight", "mamba_encoder.layers.2.out_proj_v.weight", "mamba_encoder.layers.3.A_log_a", "mamba_encoder.layers.3.D_a", "mamba_encoder.layers.3.A_log_v", "mamba_encoder.layers.3.D_v", "mamba_encoder.layers.3.in_proj_a.weight", "mamba_encoder.layers.3.in_proj_v.weight", "mamba_encoder.layers.3.conv1d_a.weight", "mamba_encoder.layers.3.conv1d_a.bias", "mamba_encoder.layers.3.conv1d_v.weight", "mamba_encoder.layers.3.conv1d_v.bias", "mamba_encoder.layers.3.x_proj_a.weight", "mamba_encoder.layers.3.x_proj_v.weight", "mamba_encoder.layers.3.dt_proj_a.weight", "mamba_encoder.layers.3.dt_proj_a.bias", "mamba_encoder.layers.3.dt_proj_v.weight", "mamba_encoder.layers.3.dt_proj_v.bias", "mamba_encoder.layers.3.attention.in_proj_weight", "mamba_encoder.layers.3.attention.in_proj_bias", "mamba_encoder.layers.3.attention.out_proj.weight", "mamba_encoder.layers.3.attention.out_proj.bias", "mamba_encoder.layers.3.out_proj_a.weight", "mamba_encoder.layers.3.out_proj_v.weight", "mamba_encoder.layers.4.A_log_a", "mamba_encoder.layers.4.D_a", "mamba_encoder.layers.4.A_log_v", "mamba_encoder.layers.4.D_v", "mamba_encoder.layers.4.in_proj_a.weight", "mamba_encoder.layers.4.in_proj_v.weight", "mamba_encoder.layers.4.conv1d_a.weight", "mamba_encoder.layers.4.conv1d_a.bias", "mamba_encoder.layers.4.conv1d_v.weight", "mamba_encoder.layers.4.conv1d_v.bias", "mamba_encoder.layers.4.x_proj_a.weight", "mamba_encoder.layers.4.x_proj_v.weight", "mamba_encoder.layers.4.dt_proj_a.weight", "mamba_encoder.layers.4.dt_proj_a.bias", "mamba_encoder.layers.4.dt_proj_v.weight", "mamba_encoder.layers.4.dt_proj_v.bias", "mamba_encoder.layers.4.attention.in_proj_weight", "mamba_encoder.layers.4.attention.in_proj_bias", "mamba_encoder.layers.4.attention.out_proj.weight", "mamba_encoder.layers.4.attention.out_proj.bias", "mamba_encoder.layers.4.out_proj_a.weight", "mamba_encoder.layers.4.out_proj_v.weight", "mamba_encoder.layers.5.A_log_a", "mamba_encoder.layers.5.D_a", "mamba_encoder.layers.5.A_log_v", "mamba_encoder.layers.5.D_v", "mamba_encoder.layers.5.in_proj_a.weight", "mamba_encoder.layers.5.in_proj_v.weight", "mamba_encoder.layers.5.conv1d_a.weight", "mamba_encoder.layers.5.conv1d_a.bias", "mamba_encoder.layers.5.conv1d_v.weight", "mamba_encoder.layers.5.conv1d_v.bias", "mamba_encoder.layers.5.x_proj_a.weight", "mamba_encoder.layers.5.x_proj_v.weight", "mamba_encoder.layers.5.dt_proj_a.weight", "mamba_encoder.layers.5.dt_proj_a.bias", "mamba_encoder.layers.5.dt_proj_v.weight", "mamba_encoder.layers.5.dt_proj_v.bias", "mamba_encoder.layers.5.attention.in_proj_weight", "mamba_encoder.layers.5.attention.in_proj_bias", "mamba_encoder.layers.5.attention.out_proj.weight", "mamba_encoder.layers.5.attention.out_proj.bias", "mamba_encoder.layers.5.out_proj_a.weight", "mamba_encoder.layers.5.out_proj_v.weight", "mamba_encoder.layers.6.A_log_a", "mamba_encoder.layers.6.D_a", "mamba_encoder.layers.6.A_log_v", "mamba_encoder.layers.6.D_v", "mamba_encoder.layers.6.in_proj_a.weight", "mamba_encoder.layers.6.in_proj_v.weight", "mamba_encoder.layers.6.conv1d_a.weight", "mamba_encoder.layers.6.conv1d_a.bias", "mamba_encoder.layers.6.conv1d_v.weight", "mamba_encoder.layers.6.conv1d_v.bias", "mamba_encoder.layers.6.x_proj_a.weight", "mamba_encoder.layers.6.x_proj_v.weight", "mamba_encoder.layers.6.dt_proj_a.weight", "mamba_encoder.layers.6.dt_proj_a.bias", "mamba_encoder.layers.6.dt_proj_v.weight", "mamba_encoder.layers.6.dt_proj_v.bias", "mamba_encoder.layers.6.attention.in_proj_weight", "mamba_encoder.layers.6.attention.in_proj_bias", "mamba_encoder.layers.6.attention.out_proj.weight", "mamba_encoder.layers.6.attention.out_proj.bias", "mamba_encoder.layers.6.out_proj_a.weight", "mamba_encoder.layers.6.out_proj_v.weight", "mamba_encoder.layers.7.A_log_a", "mamba_encoder.layers.7.D_a", "mamba_encoder.layers.7.A_log_v", "mamba_encoder.layers.7.D_v", "mamba_encoder.layers.7.in_proj_a.weight", "mamba_encoder.layers.7.in_proj_v.weight", "mamba_encoder.layers.7.conv1d_a.weight", "mamba_encoder.layers.7.conv1d_a.bias", "mamba_encoder.layers.7.conv1d_v.weight", "mamba_encoder.layers.7.conv1d_v.bias", "mamba_encoder.layers.7.x_proj_a.weight", "mamba_encoder.layers.7.x_proj_v.weight", "mamba_encoder.layers.7.dt_proj_a.weight", "mamba_encoder.layers.7.dt_proj_a.bias", "mamba_encoder.layers.7.dt_proj_v.weight", "mamba_encoder.layers.7.dt_proj_v.bias", "mamba_encoder.layers.7.attention.in_proj_weight", "mamba_encoder.layers.7.attention.in_proj_bias", "mamba_encoder.layers.7.attention.out_proj.weight", "mamba_encoder.layers.7.attention.out_proj.bias", "mamba_encoder.layers.7.out_proj_a.weight", "mamba_encoder.layers.7.out_proj_v.weight", "mamba_encoder.layers.8.A_log_a", "mamba_encoder.layers.8.D_a", "mamba_encoder.layers.8.A_log_v", "mamba_encoder.layers.8.D_v", "mamba_encoder.layers.8.in_proj_a.weight", "mamba_encoder.layers.8.in_proj_v.weight", "mamba_encoder.layers.8.conv1d_a.weight", "mamba_encoder.layers.8.conv1d_a.bias", "mamba_encoder.layers.8.conv1d_v.weight", "mamba_encoder.layers.8.conv1d_v.bias", "mamba_encoder.layers.8.x_proj_a.weight", "mamba_encoder.layers.8.x_proj_v.weight", "mamba_encoder.layers.8.dt_proj_a.weight", "mamba_encoder.layers.8.dt_proj_a.bias", "mamba_encoder.layers.8.dt_proj_v.weight", "mamba_encoder.layers.8.dt_proj_v.bias", "mamba_encoder.layers.8.attention.in_proj_weight", "mamba_encoder.layers.8.attention.in_proj_bias", "mamba_encoder.layers.8.attention.out_proj.weight", "mamba_encoder.layers.8.attention.out_proj.bias", "mamba_encoder.layers.8.out_proj_a.weight", "mamba_encoder.layers.8.out_proj_v.weight", "mamba_encoder.layers.9.A_log_a", "mamba_encoder.layers.9.D_a", "mamba_encoder.layers.9.A_log_v", "mamba_encoder.layers.9.D_v", "mamba_encoder.layers.9.in_proj_a.weight", "mamba_encoder.layers.9.in_proj_v.weight", "mamba_encoder.layers.9.conv1d_a.weight", "mamba_encoder.layers.9.conv1d_a.bias", "mamba_encoder.layers.9.conv1d_v.weight", "mamba_encoder.layers.9.conv1d_v.bias", "mamba_encoder.layers.9.x_proj_a.weight", "mamba_encoder.layers.9.x_proj_v.weight", "mamba_encoder.layers.9.dt_proj_a.weight", "mamba_encoder.layers.9.dt_proj_a.bias", "mamba_encoder.layers.9.dt_proj_v.weight", "mamba_encoder.layers.9.dt_proj_v.bias", "mamba_encoder.layers.9.attention.in_proj_weight", "mamba_encoder.layers.9.attention.in_proj_bias", "mamba_encoder.layers.9.attention.out_proj.weight", "mamba_encoder.layers.9.attention.out_proj.bias", "mamba_encoder.layers.9.out_proj_a.weight", "mamba_encoder.layers.9.out_proj_v.weight", "mamba_encoder.norm1.weight", "mamba_encoder.norm1.bias", "mamba_encoder.norm2.weight", "mamba_encoder.norm2.bias", "hat_encoder.layers.0.mamba.layers.1.A_log_a", "hat_encoder.layers.0.mamba.layers.1.D_a", "hat_encoder.layers.0.mamba.layers.1.A_log_v", "hat_encoder.layers.0.mamba.layers.1.D_v", "hat_encoder.layers.0.mamba.layers.1.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.1.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.1.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.1.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.1.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.1.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.1.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.1.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.1.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.1.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.1.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.1.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.A_log_a", "hat_encoder.layers.0.mamba.layers.2.D_a", "hat_encoder.layers.0.mamba.layers.2.A_log_v", "hat_encoder.layers.0.mamba.layers.2.D_v", "hat_encoder.layers.0.mamba.layers.2.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.2.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.2.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.2.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.2.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.2.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.2.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.2.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.2.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.2.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.2.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.2.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.A_log_a", "hat_encoder.layers.0.mamba.layers.3.D_a", "hat_encoder.layers.0.mamba.layers.3.A_log_v", "hat_encoder.layers.0.mamba.layers.3.D_v", "hat_encoder.layers.0.mamba.layers.3.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.3.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.3.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.3.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.3.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.3.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.3.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.3.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.3.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.3.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.3.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.3.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.A_log_a", "hat_encoder.layers.0.mamba.layers.4.D_a", "hat_encoder.layers.0.mamba.layers.4.A_log_v", "hat_encoder.layers.0.mamba.layers.4.D_v", "hat_encoder.layers.0.mamba.layers.4.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.4.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.4.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.4.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.4.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.4.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.4.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.4.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.4.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.4.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.4.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.4.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.A_log_a", "hat_encoder.layers.0.mamba.layers.5.D_a", "hat_encoder.layers.0.mamba.layers.5.A_log_v", "hat_encoder.layers.0.mamba.layers.5.D_v", "hat_encoder.layers.0.mamba.layers.5.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.5.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.5.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.5.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.5.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.5.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.5.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.5.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.5.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.5.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.5.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.5.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.A_log_a", "hat_encoder.layers.0.mamba.layers.6.D_a", "hat_encoder.layers.0.mamba.layers.6.A_log_v", "hat_encoder.layers.0.mamba.layers.6.D_v", "hat_encoder.layers.0.mamba.layers.6.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.6.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.6.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.6.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.6.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.6.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.6.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.6.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.6.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.6.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.6.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.6.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.A_log_a", "hat_encoder.layers.0.mamba.layers.7.D_a", "hat_encoder.layers.0.mamba.layers.7.A_log_v", "hat_encoder.layers.0.mamba.layers.7.D_v", "hat_encoder.layers.0.mamba.layers.7.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.7.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.7.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.7.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.7.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.7.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.7.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.7.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.7.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.7.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.7.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.7.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.A_log_a", "hat_encoder.layers.0.mamba.layers.8.D_a", "hat_encoder.layers.0.mamba.layers.8.A_log_v", "hat_encoder.layers.0.mamba.layers.8.D_v", "hat_encoder.layers.0.mamba.layers.8.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.8.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.8.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.8.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.8.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.8.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.8.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.8.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.8.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.8.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.8.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.8.out_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.A_log_a", "hat_encoder.layers.0.mamba.layers.9.D_a", "hat_encoder.layers.0.mamba.layers.9.A_log_v", "hat_encoder.layers.0.mamba.layers.9.D_v", "hat_encoder.layers.0.mamba.layers.9.in_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.in_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_a.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_a.bias", "hat_encoder.layers.0.mamba.layers.9.conv1d_v.weight", "hat_encoder.layers.0.mamba.layers.9.conv1d_v.bias", "hat_encoder.layers.0.mamba.layers.9.x_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.x_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_a.bias", "hat_encoder.layers.0.mamba.layers.9.dt_proj_v.weight", "hat_encoder.layers.0.mamba.layers.9.dt_proj_v.bias", "hat_encoder.layers.0.mamba.layers.9.attention.in_proj_weight", "hat_encoder.layers.0.mamba.layers.9.attention.in_proj_bias", "hat_encoder.layers.0.mamba.layers.9.attention.out_proj.weight", "hat_encoder.layers.0.mamba.layers.9.attention.out_proj.bias", "hat_encoder.layers.0.mamba.layers.9.out_proj_a.weight", "hat_encoder.layers.0.mamba.layers.9.out_proj_v.weight". 
